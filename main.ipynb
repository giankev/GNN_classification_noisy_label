{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12016451,"sourceType":"datasetVersion","datasetId":7560003},{"sourceId":12019030,"sourceType":"datasetVersion","datasetId":7561746}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#rm -rf /kaggle/working/*","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone --branch main https://github.com/giankev/GNN_classification_noisy_label.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd hackaton/\n!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n!ls -lh datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# source/utils.py\nimport random\nimport torch\nimport numpy as np\nimport logging\nfrom typing import Dict, Any\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gzip\nimport json\nimport os\nimport pandas as pd\nfrom torch_geometric.data import Data, DataLoader\nimport torch\nfrom sklearn.model_selection import train_test_split\n\ndef load_dataset(file_path: str) -> pd.DataFrame:\n    data = []\n    db = []    \n    for file in file_path.split(' '):\n        x = os.path.basename(os.path.dirname(file))\n        with gzip.open(file, 'rt', encoding='utf-8') as f:\n            tmp = json.load(f)\n            data = data + tmp\n            db = db + [x]*len(tmp)\n    data = pd.DataFrame(data)\n    data = data.assign(db=db)\n    return data\n\ndef create_dataset_from_dataframe(df, result=True):\n    dataset = []\n    for _, row in df.iterrows():\n        edge_index = torch.tensor(row['edge_index'], dtype=torch.long)\n        edge_attr = torch.tensor(row['edge_attr'], dtype=torch.float)  \n        num_nodes = row['num_nodes']\n\n        # Safe handling of y\n        y_raw = row.get('y', None)\n        if result and y_raw is not None and isinstance(y_raw, list) and len(y_raw) > 0 and isinstance(y_raw[0], list):\n            y = torch.tensor([y_raw[0][0]], dtype=torch.long)\n        else:\n            y = torch.tensor([0], dtype=torch.long)\n\n        # Create a Data object\n        data = Data(\n            x=torch.ones((num_nodes, 1)), \n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            y=y\n        )\n\n        # Replace any NaNs with zeros\n        data.x = torch.nan_to_num(data.x, nan=0.0)\n        data.edge_attr = torch.nan_to_num(data.edge_attr, nan=0.0)\n\n        dataset.append(data)\n    return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SymmetricCrossEntropyLoss(nn.Module):\n    def __init__(self, num_classes: int = 6, alpha: float = 1.0, beta: float = 1.0, eps: float = 1e-7):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.num_classes = num_classes\n        self.eps = eps\n        self.ce = nn.CrossEntropyLoss()\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n\n        loss_ce = self.ce(logits, targets)\n\n        prob = F.softmax(logits, dim=1)\n        one_hot = F.one_hot(targets, self.num_classes).float().clamp(min=self.eps)\n        loss_rce = (- prob * torch.log(one_hot)).sum(dim=1).mean()\n\n        return self.alpha * loss_ce + self.beta * loss_rce","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NoisyCrossEntropyLoss(torch.nn.Module):\n    def __init__(self, p_noisy = 0.15):\n        super().__init__()\n        self.p = p_noisy\n        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, logits, targets):\n        losses = self.ce(logits, targets)\n        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n        return (losses * weights).mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import MessagePassing, global_mean_pool\n\n\nclass EdgeEncoder(MessagePassing):\n    def __init__(self, in_channels, edge_dim, hidden_dim):\n        super(EdgeEncoder, self).__init__(aggr='add') \n        self.node_mlp = torch.nn.Linear(in_channels + hidden_dim, hidden_dim)\n        self.edge_mlp = torch.nn.Sequential(\n            torch.nn.Linear(edge_dim, hidden_dim),\n            torch.nn.LeakyReLU(0.15),\n            torch.nn.Linear(hidden_dim, hidden_dim)\n        )\n\n    def forward(self, x, edge_index, edge_attr):\n        edge_emb = self.edge_mlp(edge_attr)  \n        return self.propagate(edge_index, x=x, edge_attr=edge_emb)\n\n    def message(self, x_i, x_j, edge_attr):\n        z = torch.cat([x_i, edge_attr], dim=1)  \n        return self.node_mlp(z)\n\n    def update(self, aggr_out):\n        return aggr_out\n    \nclass EdgeVGAE(torch.nn.Module):\n    def __init__(self, input_dim, edge_dim, hidden_dim, latent_dim, num_classes):\n        super(EdgeVGAE, self).__init__()\n        self.encoder = EdgeVGAEEncoder(input_dim, edge_dim, hidden_dim, latent_dim)\n        self.classifier = torch.nn.Linear(latent_dim, num_classes) \n        \n        self.edge_mlp = torch.nn.Sequential(\n            torch.nn.Linear(latent_dim * 2, latent_dim),\n            torch.nn.LeakyReLU(0.15),\n            torch.nn.Linear(latent_dim, edge_dim)\n        )\n\n        self.init_weights()\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, torch.nn.Linear):\n                torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=0.15)\n                if m.bias is not None:\n                    torch.nn.init.constant_(m.bias, 0)\n                    \n    def forward(self, x, edge_index, edge_attr, batch, eps=None):\n        mu, logvar = self.encoder(x, edge_index, edge_attr)\n        if eps==0.0:\n            z = mu\n        else:\n            z = self.reparameterize(mu, logvar) \n\n        class_logits = self.classifier(global_mean_pool(z, batch))  \n        return z, mu, logvar, class_logits\n\n    def reparameterize(self, mu, logvar):\n        logvar = torch.clamp(logvar, min=-10, max=10)  \n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z, edge_index):\n        adj_pred = torch.sigmoid(torch.mm(z, z.t()))\n\n        row, col = edge_index\n        edge_features = torch.cat([z[row], z[col]], dim=-1)  \n        edge_attr_pred = self.edge_mlp(edge_features)\n        edge_attr_pred = torch.sigmoid(edge_attr_pred)  \n        \n        return adj_pred, edge_attr_pred\n    \n    def recon_loss(self, z, edge_index, edge_attr):\n        adj_pred, edge_attr_pred = self.decode(z, edge_index)\n\n        # Build adjacency ground truth\n        adj_true = torch.zeros_like(adj_pred, dtype=torch.float32)\n        adj_true[edge_index[0], edge_index[1]] = 1.0  \n        adj_loss = F.binary_cross_entropy(adj_pred, adj_true)\n\n        edge_attr_pred_selected = edge_attr_pred  \n        edge_loss = F.mse_loss(edge_attr_pred_selected, edge_attr)\n        #return adj_loss\n        return 0.1*adj_loss + edge_loss\n\n    def kl_loss(self, mu, logvar):\n        logvar = torch.clamp(logvar, min=-10, max=10)  \n        return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n\nclass EdgeVGAEEncoder(torch.nn.Module):\n    def __init__(self, input_dim, edge_dim, hidden_dim, latent_dim):\n        super(EdgeVGAEEncoder, self).__init__()\n        self.conv1 = EdgeEncoder(input_dim, edge_dim, hidden_dim)\n        self.conv2 = EdgeEncoder(hidden_dim, edge_dim, hidden_dim)\n        self.drop = torch.nn.Dropout(0.05)\n\n        # Mean and log variance layers\n        self.mu_layer = torch.nn.Linear(hidden_dim, latent_dim)\n        self.logvar_layer = torch.nn.Linear(hidden_dim, latent_dim)\n\n    def forward(self, x, edge_index, edge_attr):\n        x = self.drop(x)\n        x = F.leaky_relu(self.conv1(x, edge_index, edge_attr), 0.15)\n        x = self.drop(x)\n        x = F.leaky_relu(self.conv2(x, edge_index, edge_attr), 0.15)\n        # x = self.drop(x)\n        return self.mu_layer(x), self.logvar_layer(x)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport types\n\nconfig = types.ModuleType(\"config\")\n\nimport dataclasses\nimport os\nfrom typing import Optional\n\n@dataclasses.dataclass\nclass ModelConfig:\n    test_path:  Optional[str] = None\n    train_path: Optional[str] = None\n    pretrain_paths: Optional[str] = None\n    batch_size: int = 32\n    hidden_dim: int = 128\n    latent_dim: int = 8\n    num_classes: int = 6\n    epochs: int = 50\n    learning_rate: float = 0.00005\n    num_cycles: int = 5\n    warmup: int = 5\n    early_stopping_patience: int = 20\n\n    @property\n    def folder_name(self) -> str:\n        files = self.train_path if self.train_path is not None else self.test_path\n        db = ''\n        for file in files.split(' '):\n            db += os.path.basename(os.path.dirname(file))\n        return db\n\nconfig.ModelConfig = ModelConfig\n\nsys.modules[\"config\"] = config","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nimport os\nfrom datetime import datetime\nfrom typing import List, Dict\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom torch_geometric.loader import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\n\n\ndef warm_up_lr(epoch, num_epoch_warm_up, init_lr, optimizer):\n    for params in optimizer.param_groups:\n        params['lr'] = (epoch+1)**3 * init_lr / num_epoch_warm_up**3\n\nclass ModelTrainer:\n    def __init__(self, config: 'ModelConfig', device: str):\n        self.config = config\n        self.device = device\n        self.models: List[str] = []\n        self.pretrain_models: List[str] = []\n        self.best_f1_scores = []\n\n        self._setup_directories()\n        self._setup_logging()\n        self.criterion = NoisyCrossEntropyLoss()\n\n    def predict(self, model, device, loader):\n        model.eval()\n        y_pred = []\n        with torch.no_grad():\n            for data in loader:\n                data = data.to(device)\n                z, mu, logvar, class_logits = model(data.x, data.edge_index, data.edge_attr, data.batch, eps=0.0)\n                pred = class_logits.argmax(dim=1)\n                y_pred.extend(pred.tolist())\n        return y_pred\n\n    def _setup_directories(self):\n        self.output_dir = '/kaggle/working/output'\n        os.makedirs(self.output_dir, exist_ok=True)\n        for d in ['file_checkpoints', 'best', 'file_log']:\n            os.makedirs(os.path.join(self.output_dir, d), exist_ok=True)\n\n\n    def _setup_logging(self):\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        log_filename = os.path.join(self.output_dir, 'file_log', f'training_{self.config.folder_name}_{timestamp}.log')\n    \n        for handler in logging.root.handlers[:]:\n            logging.root.removeHandler(handler)\n    \n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s [%(levelname)s] %(message)s',\n            handlers=[\n                logging.FileHandler(log_filename, mode='w'),\n                logging.StreamHandler()\n            ]\n        )\n        \n        logging.info(\"Logger created.\")\n\n    def load_pretrained(self):\n        if self.config.pretrain_paths is not None:\n            path = self.config.pretrain_paths\n        if path.endswith('.pth'):\n            self.pretrain_models = [path]\n        else:\n            with open(path, 'r') as f:\n                self.pretrain_models = [line.strip() for line in f if line.strip()]\n\n    def evaluate_model(self, model: torch.nn.Module, data_loader: DataLoader) -> Dict[str, float]:\n        model.eval()\n        total_loss, total_samples = 0.0, 0\n        all_preds, all_labels = [], []\n    \n        with torch.no_grad():\n            for data in data_loader:\n                data = data.to(self.device)\n                _, _, _, logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n                loss = self.criterion(logits, data.y)\n                preds = logits.argmax(dim=1).cpu().numpy()\n                labels = data.y.cpu().numpy()\n    \n                all_preds.extend(preds)\n                all_labels.extend(labels)\n    \n                batch_size = data.y.size(0)\n                total_loss += loss.item() * batch_size\n                total_samples += batch_size\n    \n        avg_loss = total_loss / total_samples\n        f1 = f1_score(all_labels, all_preds, average='weighted')\n        acc = accuracy_score(all_labels, all_preds)\n        return {\n            'cross_entropy_loss': avg_loss,\n            'f1_score': f1,\n            'accuracy': acc,\n            'num_samples': total_samples\n        }\n\n\n\n\n    def train_single_cycle(self, cycle_num: int, train_data, val_data):\n\n\n        model = EdgeVGAE(1, 7, self.config.hidden_dim,\n                         self.config.latent_dim,\n                         self.config.num_classes).to(self.device)\n\n         # Load pretrained models if any\n        if len(self.pretrain_models)>0:\n            n = len(self.pretrain_models)\n            model_file = self.pretrain_models[(cycle_num-1)%n]\n            model_data = torch.load(model_file, weights_only=False,map_location=torch.device(self.device))\n            model.load_state_dict(model_data['model_state_dict'])\n            logging.info(f\"Loaded pretrained model: {model_file}\")\n\n\n        train_loader = DataLoader(train_data, batch_size=self.config.batch_size, shuffle=True)\n        val_loader = DataLoader(val_data, batch_size=self.config.batch_size, shuffle=False)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=self.config.learning_rate)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            mode='max',   \n            factor=0.6,  \n            patience=7,  \n            min_lr=1e-6\n        )\n        warmup_epochs = self.config.warmup\n        best_val_loss, best_f1, epoch_best = float('inf'), 0.0, 0\n        best_model_path = None\n\n        for epoch in range(self.config.epochs):\n            if epoch < warmup_epochs:\n                warm_up_lr(epoch, warmup_epochs, self.config.learning_rate, optimizer)\n       \n\n            train_loss, train_acc = self.train_epoch(model, train_loader, optimizer)\n            val_metrics = self.evaluate_model(model, val_loader)\n            val_loss = val_metrics['cross_entropy_loss']\n            val_acc = val_metrics['accuracy']\n            val_f1 = val_metrics['f1_score']\n            \n            if (epoch + 1) % 10 == 0:\n                logging.info(\n                    f\"[Epoch {epoch + 1}] \"\n                    f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n                    f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n                )\n        \n            # Checkpoint ogni 5 epoche\n            if (epoch + 1) % 5 == 0:\n                ckpt_path = os.path.join(self.output_dir, 'file_checkpoints', f'ckpt_cycle_{cycle_num}_epoch_{epoch + 1}.pth')\n\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'epoch': epoch,\n                    'val_loss': val_loss,\n                    'val_f1': val_f1,\n                    'train_loss': train_loss,\n                    'config': self.config\n                }, ckpt_path)\n      \n            \n            print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n\n\n            if epoch >= warmup_epochs:\n                scheduler.step(val_metrics['f1_score'])\n\n            if val_metrics['f1_score'] > best_f1:\n                best_val_loss = val_metrics['cross_entropy_loss']\n                best_f1 = val_metrics['f1_score']\n                epoch_best = epoch\n\n                best_model_path = os.path.join(\n                    self.output_dir, 'best',\n                    f\"best_model_{self.config.folder_name}_cycle_{cycle_num}.pth\"\n                )\n\n\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'epoch': epoch,\n                    'val_loss': best_val_loss,\n                    'val_f1': best_f1,\n                    'train_loss': train_loss,\n                    'config': self.config\n                }, best_model_path)\n\n\n            if (epoch - epoch_best) > self.config.early_stopping_patience // 2 and epoch % 10 == 0:\n                checkpoint = torch.load(best_model_path, weights_only=False, map_location=self.device)\n                model.load_state_dict(checkpoint['model_state_dict'])\n\n            if (epoch - epoch_best) > self.config.early_stopping_patience:\n                break\n\n        self.models.append(best_model_path)\n        return best_val_loss, best_f1, best_model_path\n\n    def train_epoch(self, model, train_loader, optimizer):\n        model.train()\n        total_loss, total_samples = 0.0, 0\n        correct = 0\n    \n        for data in tqdm(train_loader, desc=\"Training\", leave=False):\n            data = data.to(self.device)\n            optimizer.zero_grad()\n    \n            z, mu, logvar, logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n            recon_loss = model.recon_loss(z, data.edge_index, data.edge_attr)\n            kl_loss = model.kl_loss(mu, logvar)\n            class_loss = self.criterion(logits, data.y)\n    \n            loss = 0.15 * recon_loss + 0.1 * kl_loss + class_loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n    \n            preds = logits.argmax(dim=1)\n            correct += (preds == data.y).sum().item()\n    \n            batch_size = data.y.size(0)\n            total_loss += loss.item() * batch_size\n            total_samples += batch_size\n    \n        avg_loss = total_loss / total_samples\n        acc = correct / total_samples\n        return avg_loss, acc\n\n\n    def train_multiple_cycles(self, df, num_cycles=10):\n        self.load_pretrained()\n        results = []\n\n        for cycle in range(num_cycles):\n            cycle_seed = cycle + 1\n            set_seed(cycle_seed)\n\n            train_data, val_data = self.prepare_data_split(df, seed=cycle_seed)\n            val_loss, val_f1, model_path = self.train_single_cycle(cycle + 1, train_data, val_data)\n\n            results.append({'cycle': cycle + 1, 'seed': cycle_seed,\n                            'val_loss': val_loss, 'val_f1': val_f1, 'model_path': model_path})\n\n\n        model_paths_file = os.path.join(self.output_dir, f\"model_paths_{self.config.folder_name}.txt\")\n        with open(model_paths_file, 'w') as f:\n\n            f.writelines(f\"{p}\\n\" for p in self.models)\n\n        return results\n\n    def get_model_loss(self, model_path: str) -> float:\n        checkpoint = torch.load(model_path, weights_only=False, map_location=self.device)\n        return checkpoint['val_loss']\n\n    def prepare_data_split(self, df, seed=1):\n        db_lst = df.db.unique()\n        if len(db_lst) > 1:\n            df_train, df_valid = pd.DataFrame(), pd.DataFrame()\n            for db in db_lst:\n                idx = (df.db == db)\n                tmp_train, tmp_valid = train_test_split(df.loc[idx, :], test_size=0.2, shuffle=True, random_state=seed)\n                df_train = pd.concat([df_train, tmp_train], ignore_index=True)\n                df_valid = pd.concat([df_valid, tmp_valid], ignore_index=True) \n        else:\n            df_train, df_valid = train_test_split(df, test_size=0.2, shuffle=True, random_state=seed)\n\n        return create_dataset_from_dataframe(df_train), create_dataset_from_dataframe(df_valid)\n\n    def _compute_ensemble_weights(self, values: np.ndarray, use_loss=True) -> np.ndarray:\n        if use_loss:\n            weights = np.exp(-values)\n        else:\n            weights = np.exp(values)\n        return weights / np.sum(weights)\n\n    def _ensemble_predict(self, test_df, weight_by='loss'):\n        test_dataset = create_dataset_from_dataframe(test_df, result=False)\n        test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False)\n\n        all_preds, all_values = [], []\n\n        for model_path in self.models:\n            model = EdgeVGAE(1, 7, self.config.hidden_dim,\n                            self.config.latent_dim,\n                            self.config.num_classes).to(self.device)\n            checkpoint = torch.load(model_path, weights_only=False, map_location=self.device)\n            model.load_state_dict(checkpoint['model_state_dict'])\n\n            value = checkpoint['val_loss'] if weight_by == 'loss' else checkpoint['val_f1']\n            preds = self.predict(model, self.device, test_loader) \n\n            all_preds.append(preds)\n            all_values.append(value)\n\n        all_preds = np.array(all_preds)\n        all_values = np.array(all_values)\n        weights = self._compute_ensemble_weights(all_values, use_loss=(weight_by == 'loss'))\n\n\n        num_samples = all_preds.shape[1]\n        num_classes = self.config.num_classes\n        weighted_votes = np.zeros((num_samples, num_classes))\n\n        for i, preds in enumerate(all_preds):\n            for idx, pred_class in enumerate(preds):\n                weighted_votes[idx, pred_class] += weights[i]\n\n        ensemble_preds = np.argmax(weighted_votes, axis=1)\n        confidence_scores = np.max(weighted_votes, axis=1)\n\n        unique, counts = np.unique(ensemble_preds, return_counts=True)\n\n\n        return ensemble_preds, confidence_scores\n\n\n    def predict_with_ensemble(self, test_df):\n        return self._ensemble_predict(test_df, weight_by='loss')\n\n    def predict_with_ensemble_score(self, test_df):\n        return self._ensemble_predict(test_df, weight_by='score')\n\n    def predict_with_threshold(self, test_df, confidence_threshold=0.5):\n        preds, confidences = self.predict_with_ensemble(test_df)\n        filtered_preds = np.where(confidences > confidence_threshold, preds, -1)\n        return filtered_preds, confidences\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse\n\ndef get_user_input(prompt, default=None, required=False, type_cast=str):\n    while True:\n        user_input = input(f\"{prompt} [{default}]: \")\n\n        if user_input == \"\" and required:\n            print(\"This field is required. Please enter a value.\")\n            continue\n\n        if user_input == \"\" and default is not None:\n            return default\n\n        if user_input == \"\" and not required:\n            return None\n\n        try:\n            return type_cast(user_input)\n        except ValueError:\n            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")\n\n\ndef get_arguments():\n    args = {}\n    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n    args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n    args['num_cycles'] = get_user_input(\"Number cycles [default:5]\", required=False, default = 5)\n    return argparse.Namespace(**args)\n\n\ndef populate_args(args):\n    print(\"\\nArguments received:\")\n    for key, value in vars(args).items():\n        print(f\"{key}: {value}\")\n\n\n# Esegui\nif __name__ == \"__main__\":\n    args = get_arguments()\n    populate_args(args)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# main.py\nimport pandas as pd\nimport torch\nimport os\n\ndef main():\n    args = get_arguments()\n\n    if args.train_path:\n        folder = os.path.basename(os.path.dirname(args.train_path))\n    elif args.test_path:\n        folder = os.path.basename(os.path.dirname(args.test_path))\n    else:\n        raise ValueError(\"You must provide at least --train_path or --test_path.\")\n\n    if args.train_path is not None:\n        \n        default_pretrain = (\n            f\"/kaggle/working/GNN_classification_noisy_label/\"\n            f\"model_for_train/model_{folder}.pth\"\n        )\n       \n        if not os.path.exists(default_pretrain):\n            raise FileNotFoundError(\n                f\"Expected pretrained checkpoint at '{default_pretrain}', but not found.\"\n            )\n            \n        config = ModelConfig(\n            test_path=args.test_path,\n            train_path=args.train_path,\n            num_cycles=args.num_cycles,\n            pretrain_paths=default_pretrain,   \n        )\n        \n    else:\n        \n        config = ModelConfig(\n            test_path=args.test_path,\n            train_path=None,\n            num_cycles=args.num_cycles,\n            pretrain_paths=None,\n        )\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    trainer = ModelTrainer(config, device)\n\n    if args.train_path:\n        print(f\"Entering training mode on folder '{folder}' …\")\n        df_train = load_dataset(args.train_path)\n        trainer.train_multiple_cycles(df_train, args.num_cycles)\n\n        if args.test_path:\n            \n            print(\"Training complete, now running inference on test set …\")\n            df_test = load_dataset(args.test_path)\n            predictions, _ = trainer.predict_with_ensemble_score(df_test)\n\n            # Save predictions to a simple ./output folder\n            output_dir = \"./output\"\n            os.makedirs(output_dir, exist_ok=True)\n            out_csv = os.path.join(output_dir, f\"testset_{folder}.csv\")\n            pd.DataFrame({\"id\": range(len(predictions)), \"pred\": predictions}).to_csv(\n                out_csv, index=False\n            )\n            print(f\"Predictions saved to: {out_csv}\")\n    else:\n        print(f\"Entering inference mode for folder '{folder}' …\")\n\n        model_paths = []\n        for cycle in range(1, 6):  \n            path = f\"/kaggle/working/GNN_classification_noisy_label/best_models/{folder}/best_model_{folder}_cycle_{cycle}.pth\"\n            if not os.path.exists(path):\n                raise FileNotFoundError(f\"Expected checkpoint at '{path}', but not found.\")\n            model_paths.append(path)\n        \n        trainer.models = model_paths\n\n        if not args.test_path:\n            raise ValueError(\"Inference mode requires --test_path to be provided.\")\n\n        df_test = load_dataset(args.test_path)\n        print(\"Generating predictions on test set …\")\n        predictions, _ = trainer.predict_with_ensemble_score(df_test)\n\n        output_dir = \"./predictions\"\n        os.makedirs(output_dir, exist_ok=True)\n        out_csv = os.path.join(output_dir, f\"testset_{folder}.csv\")\n        pd.DataFrame({\"id\": range(len(predictions)), \"pred\": predictions}).to_csv(\n            out_csv, index=False\n        )\n        print(f\"Predictions saved to: {out_csv}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}